{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some theory\n",
    "In statistics, the logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. ~ Wikipedia \n",
    "\n",
    "\n",
    "In logistic regression we use the same method for calculating values as in linear regression, but their values are fed into sigmoid or logistic function. This limits the values between 0 and 1, so the answers like 0 == 'no' and 1 == 'yes' can be obtained. The sigmoid function is shown on the graphics below.\n",
    "\n",
    "\n",
    "![sigmoid](sigmoid.png \"Sigmoid function\")\n",
    "\n",
    "\n",
    "This function intersects the positive and negative values at 0.5 which is usually taken as a threshold for the classification. Here, these function will be refered as $h_{0}(x^{i})$\n",
    "\n",
    "The cost function however is calculated differently than in linear regression. The least squared error in case of logistic regression is non-convex function which may cause gradient descent to get stuck at local minima and do not find the minimal value of the function. The graphical interpretation that I found very informative is below.\n",
    "\n",
    "\n",
    "![convex vs non-convex](convex_non-convex.png \"Convex vs non-convex function\")\n",
    "\n",
    "\n",
    "That causes the need to use a cross-entropy as a error function. Which is described as:\n",
    "$$\n",
    "    J(w,b) = \\frac{1}{N} \\sum_{i=1}^{n} [y^{i} log(h_{0}(x^{i})) + (1 - y^{i}) \n",
    "    log(1 - h_{0}(x^{i}))]\n",
    "$$\n",
    "\n",
    "The cross-entropy simply measures average number of total bits to represent an event from Q instead of P. To put it simple it measures entropy between two probability distributions. Herein, cross-entropy finds the difference between the actual probability for training data and the predicted data by sigmoid function. \n",
    "\n",
    "To sum up the result of the function is calculated as:\n",
    "$$\n",
    "    \\hat{y} = \\frac{1}{1 + e^{-wx + b}}\n",
    "$$\n",
    "\n",
    "To calculate the optimal weights the gradient descent is used for cross-entropy rate. \n",
    "\n",
    "$$\n",
    "   J' =  \\begin{bmatrix} \\frac{\\partial J} {\\partial \\beta_{1}} \\\\ \\frac{\\partial J} {\\partial \\beta_{2}} \n",
    "   \\\\ \\vdots \\\\ \\frac{\\partial J} {\\partial \\beta_{n}} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
